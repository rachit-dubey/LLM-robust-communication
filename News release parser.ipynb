{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc574ace",
   "metadata": {},
   "source": [
    "This code goes over news releases of different public-facing agencies and randomly extracts some of those news releases -- it is quite customizable, although note that for each agency, we need to write custom codes since each government website has its own bugs and protection issues..\n",
    "\n",
    "The below code goes over news releases by NOAA, EPA, and Cal Air Board "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd689d63",
   "metadata": {},
   "source": [
    "# NOAA headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ac1156",
   "metadata": {},
   "source": [
    "Parsing NOAA headline is relatively easy: https://www.noaa.gov/news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d1fd8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Scraped 300 total headlines, saved 50 random ones to 'noaa_headlines.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "def extract_articles_from_page(page_num):\n",
    "    url = f\"https://www.noaa.gov/news?page={page_num}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all(\"article\", class_=\"node-teaser\")\n",
    "    results = []\n",
    "\n",
    "    for a in articles:\n",
    "        title_tag = a.find(\"div\", class_=\"title\")\n",
    "        if title_tag:\n",
    "            link_tag = title_tag.find(\"a\")\n",
    "            if link_tag and link_tag.text and link_tag[\"href\"]:\n",
    "                title = link_tag.text.strip()\n",
    "                href = link_tag[\"href\"]\n",
    "                full_url = \"https://www.noaa.gov\" + href if href.startswith(\"/\") else href\n",
    "                results.append((title, full_url))\n",
    "    return results\n",
    "\n",
    "# Step 1: Sample 30 random pages (assume ~12-15 articles per page → ~400 total)\n",
    "max_pages = 71\n",
    "random_pages = random.sample(range(max_pages), 30)\n",
    "\n",
    "collected = set()\n",
    "for page_num in random_pages:\n",
    "    articles = extract_articles_from_page(page_num)\n",
    "    for title, url in articles:\n",
    "        collected.add((title, url))\n",
    "    time.sleep(0.2)  # be nice to NOAA's servers\n",
    "\n",
    "# Step 2: Sample 50 from the ~400\n",
    "all_articles = list(collected)\n",
    "sampled = random.sample(all_articles, min(50, len(all_articles)))\n",
    "\n",
    "# Step 3: Save to CSV\n",
    "with open(\"noaa_headlines.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for title, url in sampled:\n",
    "        writer.writerow([\"NOAA\", title, url])\n",
    "\n",
    "print(f\" Scraped {len(all_articles)} total headlines, saved 50 random ones to 'noaa_headlines.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2153d7",
   "metadata": {},
   "source": [
    "# Cal Air board headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b84b3",
   "metadata": {},
   "source": [
    "Note: Cal Air website is terribly buggy and most news releases on pages 2 onwards are not accessible.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "172b03e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 21 headlines from CARB front page to 'carb_headlines.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "BASE_URL = \"https://ww2.arb.ca.gov\"\n",
    "URL = f\"{BASE_URL}/news\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Fetch and parse the first news page\n",
    "r = requests.get(URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "# Inspect and update the selector if needed\n",
    "links = soup.select(\"a[href^='/news/']\")\n",
    "\n",
    "results = []\n",
    "for tag in links:\n",
    "    title = tag.text.strip()\n",
    "    href = tag.get(\"href\")\n",
    "    if title and href and href.startswith(\"/news/\"):\n",
    "        full_url = BASE_URL + href\n",
    "        results.append((title, full_url))\n",
    "\n",
    "# Deduplicate\n",
    "unique_results = list(set(results))\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"carb_headlines.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    for title, url in unique_results:\n",
    "        writer.writerow([\"CARB\", title, url])\n",
    "\n",
    "print(f\" Saved {len(unique_results)} headlines from CARB front page to 'carb_headlines.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e9077",
   "metadata": {},
   "source": [
    "# EPA headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2939923b",
   "metadata": {},
   "source": [
    "Note: EPA has bot protection and we CANNOT parse the webpages, have to save them locally to parse the headlines.. sigh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a0c7f0",
   "metadata": {},
   "source": [
    "First locally save 10 random pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1106d699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved HTML pages to: epa_pages\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "NUM_PAGES = 10\n",
    "MIN_PAGE = 31\n",
    "MAX_PAGE = 100  # adjust if needed\n",
    "BASE_URL = \"https://www.epa.gov/newsreleases/search?page=\"\n",
    "SAVE_DIR = \"epa_pages\"\n",
    "\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "page_numbers = random.sample(range(MIN_PAGE, MAX_PAGE), NUM_PAGES)\n",
    "\n",
    "for page_num in page_numbers:\n",
    "    url = BASE_URL + str(page_num)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    html = driver.page_source\n",
    "    with open(f\"{SAVE_DIR}/epa_page{page_num}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html)\n",
    "\n",
    "driver.quit()\n",
    "print(\"Saved HTML pages to:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0402e38f",
   "metadata": {},
   "source": [
    "Next parse and save 50 random headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac6c9c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: Saved epa_headlines.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set to your local directory\n",
    "SAVE_DIR = \"epa_pages\"\n",
    "\n",
    "# Find all relevant files in that folder\n",
    "epa_files = [os.path.join(SAVE_DIR, f) for f in os.listdir(SAVE_DIR)\n",
    "             if f.startswith(\"epa_page\") and f.endswith(\".html\")]\n",
    "\n",
    "headlines_data = []\n",
    "\n",
    "# Parse each file for headlines and URLs\n",
    "for filepath in epa_files:\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "        for tag in soup.select(\"h3 a\"):\n",
    "            headline = tag.get_text(strip=True)\n",
    "            link = tag.get(\"href\")\n",
    "            full_url = \"https://www.epa.gov\" + link if link.startswith(\"/\") else link\n",
    "            headlines_data.append([\"EPA\", headline, full_url])\n",
    "\n",
    "# Randomly select 50\n",
    "sampled = random.sample(headlines_data, min(50, len(headlines_data)))\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"epa_headlines.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    #writer.writerow([\"source\", \"headline\", \"url\"])\n",
    "    writer.writerows(sampled)\n",
    "\n",
    "print(\"Done: Saved epa_headlines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35f70e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
